 \chapter{Envrionment.yaml Machine Specification}
 \label{chap:environment_file_spec}

A Environment.yaml file describes aspects and resources of specific machines
that allow SMARTS to run tests across a multitude of machines. Each machine
that will be used by SMARTS will need to have a corresponding Environment.yaml
file, which will most likely be named that machines name followed by a .yaml
file extension.

So for instance, the Cheyenne super computer's Environment.yaml file would be
cheyenne.yaml, while Casper's would be casper.yaml.

The information within an Environment.yaml file includes: whether the machine
is an HPC or not; what type of HPC scheduler it uses (if any); the number of
cores it uses; and compilers; libraries and MPI implementations.

An Environment.yaml file contains two required section, with one optional
section.

\begin{itemize}
\item Required Sections
    \begin{itemize}
        \item {\tt Description} - Describes general information about the machine
        \item {\tt Modsets} - Describes different compiler, MPI installation and library combinations
    \end{itemize}
\item Optional Sections
    \begin{itemize}
        \item {\tt PBS\_OPTIONS} - Optional default options to be passed to PBS jobs
        \item {\tt SLURM\_OPTIONS} - Optional default options to be passed to Slurm jobs
    \end{itemize}
\end{itemize}

\section{Description}
\label{section:description}

The Description sections describes the machine in general and includes
information to inform SMARTS of the machines name, the number of CPU's
available, whether or not the machine is an HPC (and if it is what type), if
the machine is to use the LMOD or Modules program and, if it is, the location
of the lmod command.

An example Description section that contains all necessary parts follows:

\begin{lstlisting}[language=yaml, 
                   caption={Examlpe Cheyenne Environment.yaml Description},
                   label={lst:cheyenne_desc_example}]
// cheyenne-description.yaml
Description:
    Name: Cheyenne
    Max Cores: 4
    Modules: True
    LMOD_CMD: /glade/u/apps/ch/opt/lmod/8.1.7/lmod/libexec/lmod
    HPC: PBS
\end{lstlisting}

The example yaml file above is the Description section for the Cheyenne super
computer. According to the description above, SMARTS will know that: it can
load libraries, compilers and MPI implementations via the module command (see
specifying libraries below); that Cheyenne is a super computer and uses the PBS
scheduler; and that SMARTS can use up to four CPUs on the login nodes on
Cheyenne.

NOTE: The {\tt Max Cores} option in the Description specifies the number of
maximum cores that can be used in the location where SMARTS is ran. So, if
SMARTS is ran on the login node of Cheyenne, and {\tt Max Cores} is set to
{\tt 4}, then SMARTS will only use that many CPUs; however, this does not mean
that a test cannot use more than 4 CPUs. A test could, for instance, launch a
job via the HPC's batch node and use more than the specified amount in {\tt Max
Cores}.

\section{HPC Options}
\label{sec:hpc_options}

The \hpcoptions section of the Environment.yaml file specifies default options
that can be passed to an HPC instance. For instance, a user might want to
always have the workload manager send an email on a job completion to their
email, or they may want to always run under a specific account key. If they do,
they can specify those options and arguments in the \hpcoptions.

The \hpcoptions can contain any options that would be use in the job script for
that machine's workload manager.

Tests will need to retrieve the \hpcoptions dictionary from their environment
class instance and pass it to {\tt HPC.launch\_job}. Tests can also edit
existing dictionary items or add new ones by editing the \hpcoptions
dictionary.

\begin{lstlisting}[language=yaml, 
                   caption={Examlpe HPC\_Options},
                   label={lst:hpc_options}]
HPC_OPTIONS:
  M: email@gmail.com 
  q: regular
  j: oe
\end{lstlisting}


\section{Modsets}
\label{sec:modsets}

The Modsets section describes compilers, MPI implementations and libraries.
Because Fortran libraries most often need to be built with the compilers they
are built with, modsets are used to describe combination of a compiler, MPI
implementation and any number of libraries and/or environment variables.

A installation of a compiler, MPI implementation, or library can be described
as either a module or as a environment variable combination. 

% TODO: Update this to use \ref
Listing \ref{lst:intel_modset_example} contains an example Modset section with
a single modset for a {\tt INTEL-19.0.1} compiler, this specific example is
take from the Cheyenne.yaml environment file.

\begin{lstlisting}[language=yaml, 
                   caption={Example Cheyenne Intel Modset},
                   label={lst:intel_modset_example}]
// cheyenne-intel-modset.yaml
Modsets:
  ###############
  # INTEL-19.0.2
  ###############
  INTEL-19.0.2:
    Name: intel-19.0.2
    Compiler:
      Name: intel
      Version: 19.0.2
      Module: intel
      Executables:
        - ifort
        - icc
    MPI:
      Module: mpt
      Version: 2.19
      Executables:
        - mpicc
        - mpif90
    Libs:
      - p-netcdf:
        Name: PNETCDF
        Value: /glade/work/duda/libs-intel19.0.2
      - c-netcdf:
        Name: NETCDF
        Value: /glade/work/duda/libs-intel19.0.2
      - pio:
        Name: PIO
        Value: /glade/work/duda/libs-intel19.0.2
      - external_libs:
        Name: MPAS_EXTERNAL_LIBS
        Ealue: "-L${NETCDF}/lib -lhdf5_hl -lhdf5 -ldl -lz"
      - external_includes:
        Name: MPAS_EXTERNAL_INCLUDES
        Value: "-I${NETCDF}/include"
      - JASPERLIB:
        Name: JASPERLIB
        Value: "/glade/u/home/wrfhelp/UNGRIB_LIBRARIES/lib"
      - JASPERINC:
        Name: JASPERINC
        Value: /glade/u/home/wrfhelp/UNGRIB_LIBRARIES/include
      - use_pio2:
        Name: USE_PIO2
        Value: 'true'
      - precision:
        Name: PRECISION
        Value: single
\end{lstlisting}

Like the {\tt INTEL-19.0.1} modset in Listing \ref{lst:intel_modset_example}
modsets can contain three sections: the {\tt Compiler}, {\tt MPI}, and {\tt
Libs} section.  A modset is required to contain the {\tt Compiler} and {\tt
Libs} sections, but the {\tt MPI} section is optional.

\subsection{Compilers}
\label{subsec:modset_compilers}

The compiler section of a modset describes information needed for tests to load
and use a specific compiler. It includes the compilers: name (\name),
version (\version), a list of compiler executables (\executables) and
either the module name (\module), or the path to the compilers
installation directory (\path).

While the \name keyword is required in the Compiler section, it is not actively 
used by SMARTS, but provides readability for users and for tests.

The \version keyword serves two purposes. First, if \version is specified with
the \module keyword, the name specified with the \module keyword and the
version number specified with the \version keyword are combined to load a
specific lmod module version of the compiler. The second, \version is used in
conjunction with \executables keyword to confirm the correct compiler has been
loaded. See Chapter \ref{chap:environment_class} for more information on how
the executables section is used when loading a modset.

In the {\tt Intel-19.0.2} modset from Listing \ref{lst:intel_modset_example},
the compiler is specified with the name intel, and, if loaded, will be loaded
by using the lmod module Python command to load intel/19.0.2. This occurs in
the same way one would load intel/19.0.2 using the module command via the
command line: {\tt module load intel/19.0.2}. Lastly, when loaded, SMARTs will
test to see if the correct version of {\tt ifort} and {\tt icc}, which are
listed in the executables, have been loaded correctly.

There must only be one compiler section per modset.

\subsection{MPI}
\label{subsec:modset_mpi}

The \mpi section specifies an MPI installation and is specified in the same
manner specified in the same manner as a compiler will be specified. Upon being
loaded, the executables listed in \executables will be tested against the
version listed in the \version keyword.

The \mpi section is not required to be present in a modset.

\subsection{Libs}
\label{subsec:modset_libs}

The \libs section can contain information on the installation of libraries and
environment variables. The implementation of the \libs is meant to be flexible
and allow tests to access libraries and needed environment variables.

Individual entries to the \libs section can specified as either a module
(Listing \ref{lst:example_library_module}) or an environment variable (Listing
\ref{lst:example_library_env_var}). Library can either be specified as with
the \module and optional \version keywords as seen in Listing
\ref{lst:example_library_module} or as a environment variable using the \name
and \valueenv keywords as seen in Listing \ref{lst:example_library_env_var}.

\begin{lstlisting}[language=yaml, 
                   caption={Example Library Module},
                   label={lst:example_library_module}]
- netcdf:
  Module: netcdf
  Version: 3.6.3
\end{lstlisting}

\begin{lstlisting}[language=yaml, 
                   caption={Example Library Environment Variable},
                   label={lst:example_library_env_var}]
- netcdf:
  Name: NETCDF
  Value: /glade/work/duda/libs-intel19.0.2
\end{lstlisting}

The \libs section is required.
